\section{Unsupervised training and outdoor localization}

In this section, we are interested in applied our method for outdoor localization. However, accurate dense depth map of outdoor environment for supervised depth from monocular \ac{cnn} training are not easy to obtain. Thus, we decide to train our model in a unsupervised manner, \ie without ground truth depth maps as training data. In the following, we first compare the impact of unsupervised training for indoor localization and then we test our proposal on outdoor scenes.

\label{subseq:outdoor}
\subsection{Unsupervised depth from monocular training}
\paragraph{Training.} To learn depth from RGB in a unsupervised manner, we follow the training procedure of~\citep{Zhou2017a}, using the ground truth relative pose between images and by adding SSIM loss function for radiometric comparison as in~\citep{Mahjourian2018}. We train the network with Adam optimizer, learning rate of $10^{-4}$ divided by two every 5, epochs. Training takes the same time as the supervised training, with a batch size reduced to 12.

\paragraph{Unsupervised depth from monocular at scale.} It is not self-explanatory to claim that the depth maps produced from our unsupervised trained network~\citep{Zhou2017a} are at a real scale.  In~\citep{Zhou2017a}, authors use an auxiliary relative pose estimation network to make their method trainable with video sequences without any pre-processing. The counterpart is that the final CNN produces depth maps up to an unknown scale factor. Nevertheless, in our experiment they are at truth scale as we use the absolute 6-DoF camera pose (obtained by \ac{sfm} or dense 3D fusion method~\citep{Whelan2015}) to compute the relative position and orientation of the training images. Some learned depth maps can be found in figure~\ref{fig:depth_map_indoor}, showing that unsupervised method leads to true scale depth values as long as it has been trained with true camera pose information.

\input{4_pose_refinement/tabs/unsup_vs_sup}
\input{4_pose_refinement/figures/results/indoor_depth_maps}
\subsection{Comparison with fully-supervised training}
Localization on indoor environment for the supervised and unsupervised training are shown in table~\ref{tab:sup_vs_unsup}. We observe an average relative improvement of $\times$2.8/$\times$3.5, respectively $\times$1.8/$\times$2.1, for the supervised, respectively unsupervised, model in position/rotation from initial to \ac{pnlp} refined pose.  Our unsupervised-trained proposal produces comparable localization as Relocnet and outperform Posenet baseline (see table~\ref{tab:7_scenes}). These encouraging results prove that, even without ground truth depth maps as supervision data, we can apply our refinement algorithm successfully.

For the novel scenes of the 12 scenes dataset, we observe an average relative improvement of $\times$1.2/$\times$1.5, compare to $\times$1.1/$\times$1.6, for the supervised, respectively unsupervised, model in position/rotation from initial to refined pose. We also demonstrate, in figure~\ref{fig:depth_map_indoor}, the generalization capability of methods trained with or without ground truth depth maps, from images taken on both known and unknown scenes. We notice that the poor localization performance on the Apt2-bed scenes is closely related to the poor generated depth map on this scene (see figure~\ref{fig:depth_map_indoor}, two last columns).

\subsection{Outdoor localization}
\paragraph{Dataset.} We use the Cambridge Landmarks~\citep{Kendall2015} dataset for outdoor evaluation. This dataset is composed of 6 scenes featuring dynamic changes (pedestrian and cars in movement during the acquisition) acquired by a cell-phone camera. As not ground truth depth maps are available for the Cambridge Landmarks scenes, we only perform outdoor experiments related to the unsupervised depth from monocular training.

\paragraph{Network architecture and training.} For the unsupervised scenario, we use a slightly different network architecture, composed of recurrent cells (\ac{lstm}) in the decoder to capture long term dependencies~\citep{Visin2015, Li2016b} (more details can be found in appendix~\ref{apx:}). During training and testing, images are rescaled to $224 \times 112$ pixels.

\input{4_pose_refinement/tabs/outdoor}
\paragraph{Results.} Outdoor localization results are presented in table~\ref{tab:outdoor}. \ac{pnlp} performs well on outdoor scene, with a mean improvement of $\times$1.5/$\times$1.6 in position/rotation precision over initial pose given by \ac{cbir}. Our method is not able to recover a proper pose for the scene Street. As same as for the indoor failure case, this is the result of a poor initial pose estimation at the \ac{cbir} preliminary step. Compared to Posenet~\citep{Kendall2017}, our method is marginally less precise but requires only one trained model compared to the 6 models needed by Posenet and can potentially be used on unknown scenes according to the previous indoor experiments. We do not compare our method to Relocnet~\citep{Purkait2018} baseline because authors do not evaluate Relocnet on outdoor scenes and the source code is not yet available.

