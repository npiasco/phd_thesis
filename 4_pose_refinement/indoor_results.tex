\section{Indoor localization}
\label{sec:indoor}

In this section we present more detailed results on indoor localization, by comparing our method with state-of-the-art competitor and by evaluating our proposal on different environments than the one used for training.

\subsection{Competitors}
Indoor localisation error on 7 scenes~\citep{Shotton2013} dataset are presented in table~\ref{tab:7_scenes}. We compare our proposal with Relocnet~\citep{Purkait2018} and Posenet~\citep{Kendall2017} trained with a geometric-aware loss. Compared to Posenet~\citep{Kendall2017} our model is using the same trained network for all the 7 scenes, compared to one network by scene for Posenet. Relocnet relies on two different networks: one trained especially to produce discriminative global image descriptors for \ac{cbir} and the second to estimate the relative pose between two images. Our method is lighter as it uses a single network and do not uses specific training for the task of global image description.

\input{4_pose_refinement/tabs/7_scenes}
\input{4_pose_refinement/figures/results/res_visualization}
\subsection{Results}
At first glance, we find that the initial pose estimation with image retrieval produces decent results (first column), while the network used to produce the global image descriptor has not been trained to this particular task. After applying our \ac{pnlp} pose refinement, our method produces the most precise localization among the presented methods. 

Figures~\ref{fig:res_visualization1}-\ref{fig:res_visualization2} present estimated position at different steps of our method for 4 scenes. Our method is able to recover accurate positions even if there are not previous acquisitions close to the ground truth camera pose (see figure~\ref{fig:res_visualization1}, top of the fire scene or figure~\ref{fig:res_visualization2}, right of the heads scene).

We observe a failure case of our method for the scene stairs due to a poor initial pose estimation. This scene contains repetitive visual patterns that may confuse the \ac{cbir} localization.

\input{4_pose_refinement/tabs/12_scenes}
\input{4_pose_refinement/figures/results/res_visualization_gene}
\subsection{Generalization}
We report on table~\ref{tab:12_scenes} localization error the 12 Scenes dataset~\citep{Valentin2016}. The 12 scenes dataset is used to evaluate the generalization capability of our method.  For these experiments, we use the same network as mentioned earlier, trained on 7 Scenes dataset~\citep{Shotton2013}. We observe an average relative improvement of $\times$1.2/$\times$1.5 in position/rotation from initial to refined pose, compare to $\times$2.8/$\times$3.5 on the 7 scenes dataset. Even though the pose refinement is not as effective as previously, it shows that our system can be used on completely new indoor environments. 

We show on figure~\ref{fig:res_visualization_gene} position recovered by our method on 2 of the 12 scenes of the dataset. Our method is able to compute new position closer to the ground truth position compare to the initial image retrieval step.
