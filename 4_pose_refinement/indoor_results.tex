\section{Indoor localization}
\label{subseq:indoor}

\subsection{Competitors}
Indoor localisation error on 7 scenes~\citep{Shotton2013} dataset are presented in table~\ref{tab:7_scenes}. We compare our proposal with Relocnet~\citep{Purkait2018} and Posenet~\citep{Kendall2017} trained with a geometric-aware loss. At first glance, we find that the initial pose estimation with image retrieval produces decent results (first two columns), while the network used to produce the global image descriptor has not been trained to this particular task. After applying our PnlP pose refinement, the model trained in a fully supervised manner produces the most precise localisation among the presented methods. 

\subsection{Results}
\input{4_pose_refinement/tabs/7_scenes}
\input{4_pose_refinement/figures/results/res_visualization}
For the unsupervised setting, we found that FC and C+LSTM architectures perform equivalently on the indoor dataset, thus we present only results of the FC architecture. We observe an average relative improvement of $\times$2.8/$\times$3.5, respectively $\times$1.8/$\times$2.1, for the supervised, respectively unsupervised, model in position/rotation from initial to PnlP refined pose. Compared to Posenet~\citep{Kendall2017} our unsupervised model perform equivalently, while using the same trained network for all the 7 scenes, compared to one network by scene for Posenet. Our proposal clearly outperforms Relocnet~\citep{Balntas2018} in a supervised setting, while producing comparable localisation for the model trained in an unsupervised manner. It is important to remind that Relocnet relies on two different networks: one trained especially to produce discriminative global image descriptors for CBIR and the second to estimate the relative pose between two images. Our method is lighter as it uses a single network and do not uses specific training for the task of global image description. We observe a failure case of our method for the scene stairs due to a poor initial pose estimation. This scene contains repetitive visual patterns that may confuse the CBIR localisation.

\subsection{Generalisation} 
\input{4_pose_refinement/figures/results/res_visualization_gene}
We also report on table~\ref{tab:7_scenes} localization error on 8 scenes of the 12 Scenes dataset~\citep{Valentin2016}. For these experiments, we use the same network as mentioned earlier, trained on 7 Scenes dataset~\citep{Shotton2013}. We observe an average relative improvement of $\times$1.2/$\times$1.5, respectively $\times$1.1/$\times$1.6, for the supervised, respectively unsupervised, model in position/rotation from initial to refined pose. Even though the pose refinement is not as effective as previously, it shows that our system can be used on completely new indoor environments. We also demonstrate, in figure~\ref{fig:depth_map_indoor}, the generalization capability of our method through the depth maps produced by our networks, from images taken on both known and unknown scenes. We notice that the poor localization performance on the Apt2-bed scenes is closely related to the poor generated depth map on this scene (see figure~\ref{fig:depth_map_indoor}, two last columns).
