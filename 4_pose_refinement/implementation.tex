\section{Implementation details}
\label{subseq:implementation}

In this section, we present extensive experiments to evaluate our proposal. We consider two localization scenarios: indoor static scenes (section~\ref{subseq:indoor}) and outdoor dynamic scenes (section~\ref{subseq:outdoor}). We also divide our evaluation according to the data available to train our encoder/decoder architecture: fully supervised depth from monocular training (when ground-truth associated depth map are available during training), and unsupervised depth from monocular (when the only data available during training are video sequences with relative poses between images).

\subsection{Datasets} 
We test our method on the following indoor localization datasets: 7 scenes~\citep{Shotton2013} and 12 scenes~\citep{Valentin2016}. These datasets are composed of various indoor environments scanned with RGB-D sensors. We use the Cambridge Landmarks~\citep{Kendall2015} dataset for outdoor evaluation. This dataset is composed of 6 scenes featuring dynamic changes (pedestrian and cars in movement during the acquisition) acquired by a cell-phone camera. 6-\ac{dof} image poses and camera calibration parameters are provided for these 3 datasets. For all the experiments, reference images used for the initial pose estimation with \ac{cbir} are taken from the training split and query images are taken from the testing split of the respective datasets.

As not ground truth depth maps are available for the Cambridge Landmarks scenes, we only perform outdoor experiments related to the unsupervised depth from monocular training.

\subsection{Networks architecture and training}
For both fully supervised and unsupervised depth from monocular experiments, we use a U-Net like convolutional encoder/decoder architecture~\citep{Isola2017} with multi-scale outputs~\citep{Godard2017}. For the unsupervised scenario, we also try to add some recurrent layers (\ac{lstm}) in the decoder to capture long term dependencies~\citep{Visin2015, Li2016b}. We denote the fully convolutional architecture as \textbf{FC} and convolutional layers + recurrent layers architecture as \textbf{C+\ac{lstm}}. FC and C+\ac{lstm} encoders are identical, with 6.3M parameters, FC decoder has 16.7M parameters and C+\ac{lstm} decoder has 10.1M parameters.

During training and testing, images are resized to $224 \times 224$ pixels for indoor scenes, and $224 \times 112$ for outdoor images. The generated depth map is 4 times smaller than the RGB input. We use $L_1$ loss function for the fully supervised depth from monocular training. To learn depth from RGB in a unsupervised manner, we follow the training procedure of~\citep{Zhou2017a}, using the ground truth relative pose between images and by adding SSIM loss function for radiometric comparison as in~\citep{Mahjourian2018}. We train all the architecture with adam optimizer, learning rate of $10^{-4}$ divided by two every 50, respectively 5, epochs for the supervised, respectively unsupervised, training. Training takes approximately one day on our Nvidia Titan X GPU with a batch size is set to 24, respectively 12, for supervised, respectively unsupervised, training.

We train networks for indoor localization on the 7 scenes dataset (using only sequences from the training split). The 12 scenes dataset is used to evaluate the generalisation capability of our method. For outdoor localization, we train our two different architectures (FC and C+\ac{lstm}) on the Cambridge Landmarks dataset.

\subsection{Method parameters}
\input{4_pose_refinement/figures/results/ratio_t_knn}
We use NetVLAD layer with 64 clusters as global image descriptor for initial pose estimation. We concatenate features from the last convolutional layers of the encoder network, composed of 256 convolutional filters, resulting in a global descriptor of size 16384. Descriptor dimension can be further reduced with PCA projection~\citep{Arandjelovic2017}. We consider the 5-top retrieved candidates from the nearest neighbour search in the pose refinement process, resulting in a good trade-off between time consumption and pose estimation performances. For the final pose estimation, we use the fast \texttt{C++} PnP implementation from~\citep{Kneip2014opengv} and we set the inlier ratio threshold mentioned in section~\ref{subsec:pnlp} to 10\%.
% Local feature for the dense matching between query image and retrieved candidates are extracted from the second convolutional layer of the encoder, resulting on 64-dimensions local descriptors.