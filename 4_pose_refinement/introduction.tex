Many applications in Computer Vision and Robotics require an initial pose estimation of the visual data acquisition system: augmented reality~\citep{Arth2015}, visual odometry~\citep{Pascoe2015a}, SLAM~\citep{Milford2012} or visual servoing~\citep{Marchand2016}, to name a few. Coarse estimation provided from standard geo-localization system (\textit{e.g.} GPS) are not accurate enough for such applications, and other processing are required to initialize the system with a suitable pose. As our second principal contribution in this thesis is a pose refinement method, we save up the presentation of such methods for chapter~\ref{chap:4}.

Image-based localisation (IBL) consists in retrieving the exact 6 Degrees of Freedom (DoF) of an image query according to a known reference~\citep{Piasco2017}. IBL is involved in various computer vision and robotics tasks, such as camera relocalisation for augmented reality or SLAM mapping\citep{Meng2016}, autonomous driving~\citep{Brahmbhatt2017a}, robot or pedestrian localisation~\citep{Sattler2015}, cultural heritage~\citep{Aubry2014}, etc. 

IBL can be considered as a visual place recognition problem~\citep{Lowry2016} and solved using Content Based Image Retrieval (CBIR)~\citep{Arandjelovic2017}. Indeed, as the reference scene is described by a pool of geo-localised images, a coarse pose can be obtained by retrieving the closest reference image to the query. So far, the most successful approaches for IBL are methods matching 2D image features to a 3D reference point cloud, before using a Perspective-n-Point (PnP) algorithm to estimate the 6-DoF pose of the image query~\citep{Sattler2016a, Sattler2018}. Following these methods, new IBL systems have increased the localisation performances by relying on more and more complete and heavy geometric representation of the environment~\citep{Taira2018, Schonberger2017a}. However, when the underlying geometry of the scene is not available, or the computational resources allocated to the localisation framework are limited, such methods cannot be deployed.

With the recent advance in machine learning, \citet{Kendall2015} introduce Posenet, a new compact system that directly regresses the pose of a given query image. Although Posenet has the advantages of being lightweight and relies on only-images data, \citet{Sattler2019} show that performances of such methods are less precise than CBIR-based pose estimation~\citep{Torii2015}. They demonstrate that learned pose regression method are more likely to \textit{average} the pose of the training examples~\citep{Torii2011} rather that computing a real pose based on geometric constraints. Another disadvantage of Posenet-like methods rely on the fact that a different model has to be trained for each new scene. 

Based on these observations, we propose a new pose estimation method built on CBIR augmented by a subsequent pose refinement step, like in~\citep{Balntas2018}. We use dense correspondences from the retrieved image and the query to refine its 6-DoF pose with a PnP algorithm. In order to obtain a position at true scale (which is not the case with traditional multi-view methods~\citep{Hartley2003}), we exploit learning to reconstruct the depth map associated to the reference images~\citep{Tateno2017}. We take advantages of the recent progress in depth estimation from monocular images to train our model with or without the supervision of ground truth depth maps~\citep{Eigen2014, Godard2017, Zhou2017a}. In order to perform online IBL, we use the same neural model to compute the global image descriptor used in CBIR, the dense matching between the query and the retrieved image and to estimate the depth map associated to a single image. Thanks to this multi-task design, our system is compact and lightweight as Posenet while not necessitating the costly scene-specific training as mentioned earlier. Unlike traditional IBL method, our proposal do not requires heavy representation of the scene geometry as we exploit the capability of recent neural networks to learn the underlying structure of a scene from the radiometric appearance.

The rest of our paper is presented as follows: the next section is dedicated to a brief review of the related work, then the details of our method are presented in section~\ref{sec:method}. The obtained results with our proposal are discussed in section~\ref{seq:results}, and we finally conclude the paper in section~\ref{seq:conclusion}.
