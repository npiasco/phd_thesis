\section{\acs*{vbl} methods}

In the previous section, we have described data representation mostly used in \ac{vbl}. The current section is dedicated to the method built on this representation to perform localization. As mentioned in the introduction, it exists two main family of methods:
\begin{itemize}
	\item \Acf{cbir} for localization,
	\item 6 \ac{dof} camera pose estimation.
\end{itemize}

We can mention a last and recent family of \ac{vbl} method, named coarse to find localization, that can be seen as a combination of the two aforementioned localization systems.
\label{sec:vbl_methods}
	
\subsection{\acs*{cbir} for localization}
\input{2_visual_based_localization/figures/methods/cbir_for_localization}

\label{subsec:vbl_as_image_retrieval}
The aim of \ac{cbir} methods is to retrieve a set of data presents in the database that are similar to an input query. This is a problem related to instance retrieval~\citep{Zheng2017}. As the visual data used in \ac{vbl} are augmented with geospatial information (\textit{e.g.} a geotag associated to an image), retrieving documents comparable to the input provides an information on the possible location of the query. This localization method is three-step: description of the visual data, similarity association across the description vectors previously extracted and possible candidates re-ranking.

\acs*{cbir} for localization pipeline is illustrate in figure~\ref{fig:cbir_for_localization}.

\subsubsection{\acs*{cbir}: efficient data representation for localization}
As one of our main contribution in this thesis targets the design of a new learned global image representation for \ac{vbl}, we do not detail data description in this chapter. We refer reader to the first section of \acl{chp}~\ref{chap:3}, \acs{sec}~\ref{sec:cbir_data_for_loc}, for a comprehensive review of data representation suited for localization.

For the subsequent tasks of the localization process, we assume that we are able to produce low dimensional vectors to describe the visual data.

\subsubsection{Similarity Research}
\label{subsubsec:similarity_research}

The similarity research step involve evaluating the sameness between the request descriptor (\ie the description vector computed from the visual request we want to localize) and the reference descriptors. Comparison between descriptors is a trivial operation: it consists in a simple distance computation (with $L2$ norm or cosine similarity as usually used metric) between vectors. However, when the number of descriptors is very large, a brute-force approach cannot be considered and similarity search algorithms are employed. We describe below these approaches.

\paragraph{Pre-processing.}
Dimension reduction of descriptor is often performed to reduce matching time and memory footprint. The most used technique remains the \ac{pca}. \ac{pca} is applied on hight dimension vector, \textit{e.g.} weights extracted from CNN layers (\citep{Arandjelovic2017,Gordo2016}). \ac{pca} has also been used to reduce the size of local features aggregated vectors \citep{Kim2015,Torii2015} or global descriptors \citep{Ni2009}. Gaussian Random Projection is applied in~\citep{Sunderhauf2015a,Panphattarasap2016} and in a different work, binary locality-sensitive hashing~\citep{Sunderhauf2015} is used instead. To reinforce data consistency, whitening could be applied to final features before the similarity search~\citep{Jegou2012a,Gong2014,Tolias2016,Arandjelovic2017,Gordo2016,Radenovic2016}.

\paragraph{Nearest Neighbor Search.}
In some works, when the amount of data to compare remain acceptable, brute-force retrieval (or exact nearest neighbours retrieval) procedure can be employed to retrieve the closest neighbours. This is the case when a single vector is used to describe a document, i.e. where global descriptors are used (\S\ref{subsec:global_feature}). Global descriptor from \ac{cnn} trained for the task of image description~\citep{Babenko2014,Sunderhauf2015,Radenovic2016,Gordo2016,Arandjelovic2017} produce a global feature vector that is afterwards ranked against each vectors in the database according to its cosine distance. Other techniques based on local or hybrid features~\citep{Zamir2010,Zamir2014,Sunderhauf2015a} perform brute-force comparison, limiting the number of features that can be handled.

Exact nearest neighbor search becomes impracticable when the amount and/or dimensionality of the features are too large. Authors then turn to approximate nearest neighbor search to trade efficiency for rapidity, thus accepting some errors in the retrieved neighbors. Approximate matching involve hashing methods~\citep{Gionis1999} and quantization frameworks~\citep{Nister2006,Philbin2007,Jegou2011}. Interested readers may see~\citep{Wang2017} for more details.

Several nearest neighbor search algorithms are efficiently implemented in the \texttt{FLANN} library~\citep{Muja2009}, and in the new Facebook \texttt{FAISS} library~\citep{Johnson2017}.

\paragraph{Machine Learning matching Methods.}
Learning the distribution of the extracted features is an alternative to aforementioned nearest neighbor search methods.

SVM classifier is used in numerous works~\citep{Shrivastava2011,Cao2013,McManus2014,Aubry2014} to cast the similarity research as a classification task. \citet{Cao2013} initially cluster the database according to the resemblance of the images. On top of this graph of similar images, they trained SVM for each cluster and at query time oppose the input image to all classifiers. By selecting the data associated to the SVM reaching the higher score of classification, this approach permits to quickly retrieve a pool of similar images. In~\citep{McManus2014,Aubry2014} authors train linear classifiers on HOG descriptors to robustly retrieve similar images that present extreme appearances changes. \citet{Aubry2014} take the advantages of \ac{lda} data representation in order to avoid expensive SVM training (like hard negative mining used in~\citep{Shrivastava2011,Kim2015}). Similarly, \citet{Kim2015} train SVM classifier to predict the robustness of extracted descriptors. This improves the matching process and reduces the number of features to compare against the database.

\citet{Lu2015} introduce a \ac{mtl} layout designed for features similarity association. Works from \citet{torralba2003context} and \citet{Ni2009} present \ac{vbl} methods that are able to localize an input query among a set of predefined places. Authors embedded the recognition process into probabilistic framework, \ac{gmm} in~\citep{torralba2003context} and epitome in~\citep{Ni2009}, trained upon images representing different areas. Such paradigms allow an easy integration of additional features (such as depth information~\citep{Ni2009}).

\paragraph{Other Matching Methods.}
\citet{Stumm2015a} introduce an innovative method based on graph matching. The visual vocabulary abstraction is employed and augmented with a graph of covisibility of the visual words in images. The graph is constructed as follows: nodes represent visual word detected in images and edges are created between two nodes if they are seen together in a same image. This formulation permits integrating geometric relations between the extracted features. Authors use a graph kernel for the similarity comparison among the query graph and the database~\citep{Stumm2015,Stumm2016}. Notice that graph-based approaches are often employed when scenes are described by spatially organized semantic clues such as office furnitures~\citep{Salas-Moreno2013} or street equipments~\citep{Ardeshir2014}.

Area correlation algorithms is another approach for computing data similarity. Simple forms of correlation like Sum of Squared Difference (SSD) or Sum of Absolute Difference (SAD) have been used in \ac{vbl} to directly compare raw images~\citep{Poglitsch2015,Milford2015}. \citet{Wan2016} use PC (Phase Correlation) on images described with FT (Fourier Transform) in order to be robust to shadow artifacts. In the work of~\citep{Corke2013}, authors compare shadow invariant grey-scale images with Zero Mean Normalized Cross-Correlation (ZNCC).


\subsubsection{Candidates re-ranking}
\label{subsec:candidates_re_ranking}
Data can be processed after the similarity research to improve the final result. Post-processing methods are widely used to re-rank the candidate list, improving relevance of retrieved data.

\paragraph{Generic re-ranking.}    
Query expansion is a post-process that re-query the database after a first retrieval step to increase the recall rate~\citep{Chum2007,Chum2011,Tolias2014}. However, increasing the recall rate is not the main concern of \ac{vbl} indirect method~\citep{Sattler2012}. Indeed, as exposed in the introduction, a perfect \ac{vbl} indirect system should retrieve at first position the closest visual document present in the database. However, more suitable top ranked candidates in the list of retrieved data could benefit to a subsequent pose estimation step~\citep{Song2016}. The \ac{vbl} system presented by \citet{Cao2013} increase the diversity of retrieved images by introducing a probabilistic re-ranking on the assumption that the first ranked candidate is not a good one and by maximizing the probability that the second one is.
\label{par:ransac}
Last but not least, geometric consistency check is often used to reject wrong matching. Relative pose between the query and the database candidates is computed by considering homography or multiple-view transformation (see more details in the following \S\ref{para:pose_compute}), and candidates that produce the most consistent pose are ranked up. \citet{Philbin2007} democratize the use of spatial verification by introducing prior on the pose of the photography by assuming a top-oriented view. Authors perform spatial check hierarchically to get more flexibility between time computation and retrieval precision. The geometric transformation between the query and the candidate is usually computed with minimal algorithm embedded in random consensus, like RANSAC~\citep{Fischler1981}. There exists multiple alternatives to the classical RANSAC algorithm. PROSAC by~\citep{Chum2005}, used in~\citep{Donoser2014}, prioritize specific features during the random selection step. We can also enumerate LO-RANSAC used in~\citep{Philbin2007} and AC-RANSAC in~\citep{Qu2015,Qu2016}. Novel method F-SORT presented by \citet{Chan2016} show outstanding result both in term on matching quality and computation efficiency. Notice that these algorithms, beside improving the relevance of the retrieved candidates, can give information about the relative pose of the query. That is why numerous 6 \ac{dof} pose estimation methods, presented thereafter in \acs{sec}~\ref{subsec:fine_pose_estimation}, rely also on these techniques.

\paragraph{Specific \ac{vbl} re-ranking.}
Unlike conventional methods of object-retrieval, indirect \ac{vbl} can benefit from geo-localization information associated to the documents present in the database. As discussed earlier, this information can be used to construct structured graph for the similarity search process~\citep{Torii2011,Cao2013} or exploited to re-rank the candidates list~\citep{Zamir2010,Zamir2014,Sattler2016}. \citet{Zamir2010} introduce this geographic re-ranking after a classical image-retrieval algorithm to quickly remove irrelevant candidates. Authors go one step further in~\citep{Zamir2014} and embed the matching process within a Generalized Minimum Clique Graphs scheme to retrieve consistent candidates according to the GPS tag associated to the visual data. \citet{Sattler2016} generalize the problem of visual burstiness introduced by~\citep{Jegou2009} to a geographic level, introducing the concept of geometric burstiness. They improve the relevance of the ranked list of candidates using position and popularity meta-information of database images.

Innovative contribution from \citet{Torii2011} refine the query location with a linear interpolation in the feature space domain of the closest database images. The database is arranged with a graph representation, where images represent the nodes and the edges encode spatial relation, i.e. images that are close to each other (according to their GPS-tag) are connected. Firstly, a set of putative candidates are retrieved with a conventional quantization method, then the discrete feature space of the candidate is extended into a continuous space by linear interpolation according to their position in the graph. The exact position of the query is then guessed according to linear combination of GPS information of the database images. Although promising, this method relies on complete panorama images, limiting its range of applications.

\subsection{6 \acs*{dof} pose estimation}
\label{subsec:fine_pose_estimation}

At this point, we introduce the notion of \textbf{direct} \ac{vbl} methods that instantly recover the exact 6 \ac{dof} pose of the query according to a known reference. Compared to indirect approaches, direct methods provide a more accurate query pose to the detriment of the area coverage. Indeed, direct \ac{vbl} requires a smaller database and in some case a coarse prior information about the query pose. From this class of methods, we consider the three following approaches:
\begin{description}
	\item[Direct \ac{vbl} with prior:] these methods are built upon the assumption that we get a prior information about the query pose. The pose prior can be obtained through localization sensor (GPS~\citep{Chen2011,Arth2015,Poglitsch2015}, magnetic compass~\citep{Svarm2014,Zeisl2015,Svarm2016}) or by using an indirect \ac{vbl} system~\citep{Torii2011,Song2016,Sattler2017}.
	\item[Features to points matching:] this class of methods performs the global localization of the query by establishing correspondences between two-dimensional features extracted from an image and three-dimensional point cloud model of the environment (see figure~\ref{fig:direct}).
	\item[Pose regression approaches:] the last considered family of algorithms are methods that learn to directly regress from an input visual data to its corresponding pose. Standard regression techniques~\citep{Shotton2013} and \ac{cnn} architecture~\citep{Kendall2015} are employed to perform this task.
\end{description}

\subsubsection{Features to Points matching}
\label{subsubsec:sfm_methods}						
A widely represented family of direct method aims to regress the pose of a camera based on the analysis of a 3D point cloud reconstructed by \ac{sfm} algorithms. The principle of these methods is to establish 2D features to 3D points correspondences (F2P). In a first step, three-dimensional representation of the environment is built thanks to many images. Triangulated points within this structure are associated to the local features (most of the time SIFT vectors~\citep{Lowe2004}) extracted from all the images where the considered point is visible. At query time, local features from the image to localize are matched against the set of pre-computed 3D points. Finally, the features to points correspondences permit a 6 \ac{dof} pose estimation of the acquisition system.

These methods share a lot of similarity with indirect methods described in Section~\ref{sec:matching} and they present the same two-step pipeline:~data description and data similarity association. Yet the use of a geometrically structured database introduces interesting elements not exploitable in a classical image-retrieval scheme~\citep{Sattler2012a}.

\paragraph{Methods overview.}
Between methods based on prior information and F2P methods, works from \citet{Arth2009} present a system that recover the pose of a smart-phone camera by confronting an image to a subset of 3D points that should be visible in the query according to a prior pose information. \citet{Irschara2009} introduce the first F2P method based on \ac{sfm} environment representation. Authors perform scalable \ac{vbl} by registering the point cloud into synthetic visual documents covering the entire model. Latter improvement by \citet{Li2010} reverse the conventional process by searching from the point cloud correspondences in the image (P2F), instead of matching features from the image to points. This formulation causes an overhead in computation but is correctly handled by considering a compressed version of the \ac{sfm} model and by implementing end-conditions and rejection cases in their algorithm.

\citet{Sattler2011} consider the original features to points correspondences scheme by~\citep{Irschara2009} and introduce a Vocabulary-based Prioritized Search (VPS) inspired by BoF matching method. Subsequent works by the same authors~\citep{Sattler2012} augment the VPS framework with the points to features matching P2F~\citep{Li2010}. \citet{Li2012} show that the class of methods introduced in~\citep{Irschara2009,Li2010} can deal with large environment. Authors augment the P2F matching with hypothesis of co-occurrence of 3D points present in a close neighbourhood. Based on similar spatial observation, \citet{Sattler2015} consider visibility graph to reject wrong matchings. \citet{Heisterklaus2014} introduce MPEG compression for visual document in order to speed-up the system. In the work described in~\citep{Donoser2014}, authors use the descriptor redundancy associated to 3D points to train random ferns on the top of each points. F2P matching time requirement is by the fact greatly reduced. 

Works from~\citep{Middelberg2014,Lynen2015} tackle the problem of \ac{vbl} embedded in a mobile device with limited memory storage and computational power. To achieve real-time performances, authors in~\citep{Middelberg2014} produce a very light 3D model to track the mobile camera in an urban environment. They send at regular interval key-frames to a server that is in charge of computing the global pose of the camera regarding a pre-produced point cloud. Aligning a light relative point cloud reconstructed with \ac{sfm} to a bigger one have also been investigated in~\citep{Lu2015}. \citet{Svarm2014} consider the problem of \ac{vbl} with F2P matching as a combinatorial optimization problem and design a fast outliers rejection scheme. This promising work have been improved through~\citep{Zeisl2015,Svarm2016} contributions.

State of the art \ac{vbl} methods based on \ac{sfm} are dominated by techniques combining previously mentioned improvements~\citep{Sattler2016a}. Recent work by \citet{Feng2016a} reduce drastically the computational power requirement by considering fast point extractor and binary descriptors combined with an efficient similarity research. Authors show an order of magnitude in time reduction without any pose estimation performances deterioration.
%\citep{Chen2016} A lire about end-user issues

\paragraph{Features to Points pose estimation.}
F2P provides correspondences between 2D pixels and 3D colourized points. Defined by \citet{Hartley2003}, perspective-$n$-point (P$n$P) formulation is the most common tool to recover the absolute camera pose according to the point cloud reconstructed by \ac{sfm}.

Embedded in a random consensus scheme (see \S\ref{par:ransac}), six correspondences between the image and the 3D model are sufficient to retrieve the pose, if we have no information about the intrinsic parameters of the camera~\citep{Donoser2014,Li2010,Li2010,Heisterklaus2014}. This formulation is known as P6P and can be solved with Direct Linear Transformation (DLT \citep{Hartley2003}).

In particular cases, three correspondences between the image and the model are sufficient (P3P pose computation problem). Especially, the pose estimation problem can be reduced to a P3P formulation if the intrinsic parameters of the camera are known~\citep{Irschara2009,Middelberg2014}, or if 3 or more \ac{dof} are fixed~\citep{Zeisl2015,Qu2016}. In those particular cases, P3P solver introduced by \citet{Kneip2011} is mostly used to recover the pose. 
%Parler du truc dans \cite{Song2016}

\subsubsection{Direct pose regression}
\label{subsubsec:pose_regression}
The last class of reviewed direct methods cast \ac{vbl} as a pose regression problem. Two different kinds of regressors are employed in the literature: regression forest and \ac{cnn}.

\paragraph{Regression forest.}
In the initial works by~\citet{Shotton2013}, authors encode, thanks to RGB-D data, the global position of each pixel associated to a known environment in a regression forest. At query time, a handful of pixels from a depth camera frame are processed into the regression forest. The multiple pose hypothesis obtained for each pixel is then optimized in a random consensus to regress the camera position and orientation. This method is fast and precise and can be used on texture-less data. However, the depth information associated to each pixel is needed and the authors have to train a specific forest for each 3D scene. This initial method have been improved in~\citep{Guzman-rivera2014}, where authors take in consideration several candidates for the final pose regression obtained by trained predictors. \citet{Valentin2015} introduce mixture of Gaussian to represent the uncertainty associated with the regression forest prediction and significantly improve the 6 \ac{dof} estimation by embedding this information within the full camera pose regression step. The regression forest have been replaced by Neural Network (NN) in~\citep{Massiceti2016}, bringing slightly better result at the cost of computational overhead. \citet{Meng2016} consider only RGB images at query time. The loss in precision is compensated by a post pose refinement step based on nearest neighbours search with sparse extracted SIFT features.

Inspired by works presented above, \citet{Glocker2013} (extended version in~\citep{Glocker2015}) design a system based on regression ferns to quickly associate an RGB-D image to a binary feature. Ferns produce descriptor according to randomly initialized binary rules, and a look up table is maintained to directly associated image signature with 3D pose in the scene. Presented system is less precise that the one presented by~\citet{Shotton2013} but has the advantages of not relying on a heavy pre-processing step (i.e. the spawning of the regression forest). Along the same line, \citet{Cavallari} propose a new method based on pre-trained regression forest. This method permit to recover the pose of a RGB-D camera without prior knowledge about the 3D scene, more precisely than~\citet{Glocker2013,Glocker2015}.


\paragraph{CNN regressors.}
\label{para:cnn_regressor}
Introduced in 2015 by \citet{Kendall2015}, PoseNet consists of the fine-tuning of a \ac{cnn} for the task of localization. The network is trained upon a set of paired image/pose and automatically regress the 6 \ac{dof} pose of a camera that acquired a colour image. The pose obtained through this method is not as accurate as the pose obtained with ``classical'' direct methods \citep{Feng2016a,Sattler2016a} but provide great tolerance to changes in scale and appearance. Compared to regression forest~\citep{Valentin2015}, \ac{cnn} seems more appropriate to handle large environment and does not rely on depth information.

Recent improvement have been proposed by the original authors~\citep{Kendall2016} to integrate an uncertainty estimation in the regression process. \citet{Liu2016} integrate this \ac{cnn} architecture with only depth map information for recovering the pose of a camera in complete obscurity. The work by Walch et al.~\citep{Walch2016mastersThesis,Walch2016a} present a combination of a PoseNet~\citep{Kendall2015} with a \ac{lstm} units plugged at the output of the network in order to encode stronger spatial information from the image. This combination slightly improves the precision of the system. \citet{Jia2016} highlight the limited number of training example available for \ac{cnn} pose regressors. Though the learning transfer seems to be efficient~\citep{Kendall2015}, authors propose a new method to gather supplementary image/pose pairs for the fine-tuning step. They generate artificial images from a dense point cloud model obtained by \ac{sfm} thanks to a rendering software. Computer graphics shaders effects are added on some rendered views for simulating various illuminations. \citet{Contreras2017} exploit this CNN architecture in order to create a fixed size map that can be improved by adding new trajectories. Authors were able to reduce the original size of the CNN by factor of three while maintaining similar localization performances on indoor scenes. Recent contribution~\citep{Kendall2017} investigate new loss-functions for the training phase of the \ac{cnn}, mimicking the philosophy used in multi-view geometry standard systems~\citep{Hartley2003}.

Differently, recent work from~\citet{Weyand2016} consider the localization problem as a classification task. They perform a worldwide training on 126M images categorized into 26k places across the globe. According to a given image a \ac{cnn}, named PlaNET, estimates a map of probable location for the query. Localization of multiples photos taken from a common album can be performed by augmenting the original network with a \ac{lstm} layer. \citet{Vo2017} push further the study of such a neural network and conclude that the features extracted from layers of PlaNET are more discriminative to determine the location of an input image that the CNN classifier itself. By extracting features instead of using a classification algorithm, their contribution is closer to the original world-wide localization method IM2GPS~\citep{Hays2008}.


\subsection{Coarse to fine localization}