\section{Laser reflectance as side information}

\label{sec:modality_ref}
\input{3_side_modality_learning_for_localisation/figures/reflectance/ref_examples}

In this section we investigate the use of another modality replacing the depth map in order to evaluate the generalization capabilities of the proposed framework. We use lidar reflectance values as auxiliary modality for these experiments.

\subsection{Laser reflectance}
Lidar reflectance is defined by the proportion of the signal returned to the laser sensor after hitting an object in the scene. Reflectance characterizes the material property of an object. We use the reflectance information provided in the Robotcar dataset~\cite{Maddern2016}. Reflectance values range from 0 to 1 indicating if the object has reflected from 0 to 100\% of the original laser beam. We proceed the sparse reflectance data in the same manner as the depth map using inpainting algorithm from~\cite{Bevilacqua2017} to produce dense reflectance maps (\S~\ref{para:training_data}), and use exactly the same decoder architecture for the reflectance map and the depth map. Examples of ground truth and reconstructed dense reflectance map are presented in figure~\ref{fig:ref_examples}.

\subsection{Reflectance versus Depth}
\input{3_side_modality_learning_for_localisation/figures/reflectance/ref_vs_depth}
We report in figure~\ref{fig:ref_vs_depth} results using reflectance map during the descriptor training (\textbf{RGB(R)}, in gray). We also illustrate in figure~\ref{fig:im_exs} localization performances of the different methods by comparing the top-1 retrieved candidate after similarity evaluation. Localization accuracy is slightly worst when using the reflectance map than the results obtained while using the depth map. Still, reflectance information is beneficial as it increases the results over the RGB only descriptor. We can draw the conclusion that scene geometry is more informative for long term localization than reflectance property of observed objects.

We find that the reflectance side information signal enhances the image descriptor by leveraging visual clues of material with particular property: low reflectance capability (like windows, see figure~\ref{fig:im_exs}, 2$^{nd}$ row) or inversely very high light reflecting property (\textit{e.g.} traffic signs, see figure~\ref{fig:im_exs}, last row). In a different way, depth map training supervision provides interesting building shapes understanding (see the recognized tower building on figure~\ref{fig:im_exs}, CMU - LT 2$nd$ row).

The reflectance-augmented descriptor shows poor results on the snowy scenarios (figure~\ref{fig:ref_vs_depth} b-d). It is not surprising as the snow presents on the scene highly reflect the light, confusing our system based on material reflectance. 

\subsection{Multi-modal complementarity of Reflectance and Depth}
\input{3_side_modality_learning_for_localisation/figures/reflectance/multi_mod}
In this final experiment, we compare the performances of a single side modality training descriptor and a multiple side modalities training descriptor. We slightly modify our original system to benefit from both depth and reflectance information, by adding an extra modal branch with refectance decoder $D_a^{\prime}$ and auxiliary encoder and descriptor $\{E_a^{\prime}, P_a^{\prime}\}$. The modified network is presented in figure~\ref{fig:multi_mod}. We report localization results of the three methods, depth map as side information (\textbf{RGB(D)}, in blue), reflectance map as side information (\textbf{RGB(R)}, in gray) and depth and reflectance map as side information (\textbf{RGB(DR)}, in green), in figure~\ref{fig:ref_vs_depth}.

We do not observe systematic improvement when using both modalities. Nevertheless we obtain best localization results for 3 out of 5 query sets (figure~\ref{fig:ref_vs_depth} b, c \& e). We observe that modality combination is beneficial only if each modal information performs equivalently when used alone. In other words, if one modality is a lot more informative than the other on a specific dataset (for instance depth over reflectance for the query set CMU - Snow, figure~\ref{fig:ref_vs_depth}-d), the combination of the both will cancel potential benefit given by the most informative modality.  As discussed before, the reflectance information can be source of disturbance if snow is present on the image, resulting on a worse scene description. On figure~\ref{fig:im_exs}, we can observe successful image localization on very challenging examples: CMU - LT 1$^{st}$ row, where the closest reference image is highly overexposed and on Oxford - Snow 1$^{st}$ row with this very confounding image query.

These preliminary results concerning the use of multiple modalities during the training process of the descriptor are encouraging. Still, additional experiments have to be performed. In particular the behavior of the proposal according to the joint use of these modalities indicate that we have to focus the final descriptor fusion; modality-aware aggregation descriptor or more complex attention mechanism may be considered~\cite{Seymour2018}.