\section{\Ac{vbl} as image retrieval}
\label{sec:vbl_as_image_retrieval}
	The aim of \textbf{indirect} VLB methods is to retrieve a set of data presents in the database that are similar to an input query. This is a problem related to Content Based Image Retrieval~\citep{Zheng2017}. As the visual data used in \ac{vbl} are augmented with geospatial information (\textit{e.g.} a geotag associated to an image), retrieving documents comparable to the input provides an information on the possible location of the query. This image-retrieval-like problem is two-step: description of the visual data (for both the query and the database, see Section~\ref{sec:image_representation}) and similarity association across the description vectors previously extracted.
    
    We explore three key steps present in indirect \ac{vbl}: features aggregation, similarity research and candidates re-ranking. 
	
	\subsection{Features Aggregation}
	\label{subsec:features_aggregation}
		The similarity search can be computationally expensive when the data is described by a large descriptor, i.e. a vector of high dimension. Particularly, local features (\S\ref{subsec:local_feature}) are prompt to produce a large number of descriptors for each single data. Features aggregation is then performed in order to reduce the dimensionality of the descriptor vector. In \ac{vbl}, the aggregation process emphasize specific features that are more beneficial for the localization task.
		
		\paragraph{Quantization}
			Quantization methods have been widely adopted in image-retrieval domain since pioneer contribution of \citet{Sivic2003}. They consider the problem of object retrieval in an image described through local features in the same manner as text document research. Words equivalent in image domain becomes local features and a dictionary is build upon a large set of features extracted from visual documents' database. These features are clustered to reduce the size of the dictionary; clusters' centroids are then called visual words. For each visual word in the dictionary, and inverted file is maintained to efficiently retrieve all the data that present this specific visual word. The bag of features (BoF) associates a vector of the dimension of the dictionary containing the visual word frequency of a specific visual document. With this representation, data similarity can be efficiently computed by a simple inner product of their respective visual word frequency vector.
	       
	       \begin{description}
	       		\item[Feature to visual word assignment.] BoF original scheme~\citep{Sivic2003} proceeds to a hard assignment from the extracted feature to the nearest visual word in the dictionary. However, depending on where the feature lies inside the Voronoi cell created in the clustering step, hard assignment can deteriorate the representation of the visual document. Soft assignment~\citep{Philbin2008} methods have been considered by associating the feature according to a linear combination of the $k$ nearest visual words. Hamming embedding (HE), introduced by \citet{Jegou2008}, subdivides Voronoi cells and associates to each feature a binary signature to refine its position in the visual vocabulary. This method leads to excellent result in term of accuracy and rapidity and is still used in state-of-the-art \ac{vbl}~\cite{Arandjelovic2014}. Inspired by Fisher Vectors (FV) formulation~\citep{Perronnin2010}, \citet{Jegou2012} introduce Vector of Locally Aggregated Descriptors (VLAD) representation for image-based retrieval. The difference between feature and its closest visual word is assigned to the final descriptor, instead of the visual word itself. The underlying idea behind VLAD representation have inspired various \ac{vbl} methods~\citep{Kim2015,Torii2015,Arandjelovic2016,Yan2016}. For instance, \citet{Kim2015} introduce PBVLAD method to locally fuse SIFT features detected inside a MSER blob. Novel features aggregation method have been recently presented in~\citep{Jegou2014}.
         	
		    \item[Weighting scheme.] The weighting step is supposed to emphasize discriminative features regarding the similarity comparison.	Original method by \citep{Sivic2003} used \textit{tf-idf} weighting, relying on the occurrence frequency of the features in the database. \citet{Jegou2009} handle the problem of intra and inter burstiness of visual words (i.e. the fact that a feature is more likely to appear in an image if it has already been detected once) by adapting the weight of the visual words before (inter-burstiness) and during (intra-burstiness) the query process. \citet{Torii2013} tackle the problem of visual burstiness introduced by repetitive structures (abundant in urban environment) and introduce meta-features encompassing several similar descriptors (comparable both in their descriptor vector and their spatial location in the image). Such improvement permits a dense extraction of local features in images, bringing superior result in urban environment \ac{vbl}~\citep{Qu2016,Torii2015}. Another work from \citet{Morago2016} that exploits the redundancy present in buildings facades. Recently, \citet{Arandjelovic2014} improve \textit{tf-idf} scheme by considering the descriptors' density in feature space. With their DisLoc weighing, 7\% of the less discriminative visual words can be removed from the database without impacting the performances of the similarity computation. \citet{Mousavian2015} introduce semantic knowledge in the local feature weighting process, reducing the impact of features associated with non-relevant elements for localization (i.e. elements that are likely to change or disappear, such as trees and cars).
	       \end{description}
				
		\paragraph{Aggregation in CNN}
		\input{3_side_modality_learning_for_localisation/tabs/cnn_details}
			\label{subsubsec:cnn_aggregation}
			In \ac{cnn} based methods, feature aggregation is also a subject of study. There are two different aspects of aggregation in \ac{cnn} domain: gathering of features extracted from networks and intra-pooling of deep features within the \ac{cnn}.

			\begin{description}
				\item[Multiple features aggregation.] \ac{cnn} descriptor can be combined with local or patch detectors, in order to obtain sparse representation of the data (see table~\ref{tab:cnn_details} for examples). In this case, features extracted from the image can be gathered into a single descriptor, like in the BoF framework. VLAD embedding is employed in~\citep{Yan2016} and in~\citep{Panphattarasap2016} patches are sorted according to their relative position in the image and aggregated in a Landmark Distribution Descriptor (LDD) to improve the subsequent similarity search. \citet{Zhi2016} exploit the intensity response of each patches to discard the descriptors with the lowest intensity. In~\citep{Iscen2017}, authors create panorama features by aggregating multiple image representations (extracted from a \ac{cnn}) in a memory vector.
				
				\item[Pooling of deep feature maps.]	The meaningful representation of an image through neural network is achieved by extracting responses of convolutional layers~\citep{Babenko2014,Sunderhauf2015}. A convolutional layer can be seen as a 	feature bloc, composed of several activation maps of the same size. Considering the raw response of such a layer results in a high dimensionality feature vector. In order to capture a more discriminative image representation, several activation map pooling methods are applied. Table~\ref{tab:cnn_details} presents the different convolutional layer aggregation scheme employed in \ac{vbl}. Maximum Activation of Convolutions (MAC)~\citep{Razavian2014a} reduce the feature size by aggregating the maximum of each activation maps into unidimensional vector. Sum-Pooled Convolutional features (SPoC)~\citep{Babenko2015} shows superior results compared to MAC aggregation. Instead of computing the maximum over all the activation maps, authors simply sum the responses for each map. Regional Maximum Activation of Convolutions (R-MAC)~\citep{Tolias2016} is an improvement of the precedent MAC method, consisting of the computation of the maximum of activation over regions of various sizes on the activation map. \citet{Gordo2016} achieve state-of-the-art performances by combining R-MAC representation with a custom Region Proposal Network (RPN) that autonomously detects regions on the activation map to compute the max-pooling. An entirely trainable aggregation layer, called NetVLAD, have been proposed in~\citep{Arandjelovic2016}. Authors design a differentiable architecture that aim to mimic VLAD aggregation scheme. In combination with an adapted training framework, this architecture seems to be the best suited for \ac{vbl} tasks. \citet{Kim2017a} use the NetVLAD aggregation layer coupled with an Contextual Reweighing Network (CRN) to downgrade irrelevant features according to their local neighbourhood, without the use of any manually annotated data.
			\end{description}						
				 					
	\subsection{Similarity Research}
	\label{subsec:similarity_research}

	    Comparison between descriptors is a trivial operation: it consists in a simple distance computation (with $L2$ norm as usually used metric) between vectors. However, when the number of descriptors is very large, a brute-force approach cannot be considered and similarity search algorithms are employed. We describe below these approaches.
	    
		\paragraph{Pre-processing}
        	Dimension reduction of descriptor is often performed to reduce matching time and memory footprint. The most used technique remains the \ac{pca}. \ac{pca} is applied on hight dimension vector, \textit{e.g.} weights extracted from CNN layers (\citep{Arandjelovic2016,Gordo2016}). \ac{pca} has also been used to reduce the size of local features aggregated vectors \citep{Kim2015,Torii2015} or global descriptors \citep{Ni2009}. Gaussian Random Projection is applied in~\citep{Sunderhauf2015a,Panphattarasap2016} and in a different work, binary locality-sensitive hashing~\citep{Sunderhauf2015} is used instead. To reinforce data consistency, whitening could be applied to final features before the similarity search~\citep{Jegou2012a,Gong2014,Tolias2016,Arandjelovic2016,Gordo2016,Radenovic2016}.

		\paragraph{Nearest Neighbour Search}


			In some works, when the amount of data to compare remain acceptable, brute-force retrieval (or exact nearest neighbours retrieval) procedure can be employed to retrieve the closest neighbours. This is the case when a single vector is used to describe a document, i.e. where global descriptors are used (\S\ref{subsec:global_feature}). Global descriptor from \ac{cnn} trained for the task of image description~\citep{Babenko2014,Sunderhauf2015,Radenovic2016,Gordo2016,Arandjelovic2016} produce a global feature vector that is afterwards ranked against each vectors in the database according to its cosine distance. Other techniques based on local or hybrid features~\citep{Zamir2010,Zamir2014,Sunderhauf2015a} perform brute-force comparison, limiting the number of features that can be handled.

			Exact nearest neighbour search becomes impracticable when the amount and/or dimensionality of the features are too large. Authors then turn to approximate nearest neighbour search to trade efficiency for rapidity, thus accepting some errors in the retrieved neighbours. Approximate matching involve hashing methods~\citep{Gionis1999} and quantization frameworks~\citep{Nister2006,Philbin2007,Jegou2011}. Interested readers may see~\citep{Wang2017} for more details.
				
			Several nearest neighbour search algorithms are implemented in the \texttt{FLANN} library~\citep{Muja2009}, and in the new Facebook \texttt{FAISS} library~\citep{Johnson2017}. 
				
		\paragraph{Machine Learning Methods}
			Learning the distribution of the extracted features is an alternative to aforementioned nearest neighbour search methods.
			
    	    SVM classifier is used in numerous works~\citep{Shrivastava2011,Cao2013,McManus2014,Aubry2014} to cast the similarity research as a classification task. \citet{Cao2013} initially cluster the database according to the resemblance of the images. On top of this graph of similar images, they trained SVM for each cluster and at query time oppose the input image to all classifiers. By selecting the data associated to the SVM reaching the higher score of classification, this approach permits to quickly retrieve a pool of similar images. In~\citep{McManus2014,Aubry2014} authors train linear classifiers on HOG descriptors to robustly retrieve similar images that present extreme appearances changes. \citet{Aubry2014} take the advantages of \ac{lda} data representation in order to avoid expensive SVM training (like hard negative mining used in~\citep{Shrivastava2011,Kim2015}). Similarly, \citet{Kim2015} train SVM classifier to predict the robustness of extracted descriptors. This improves the matching process and reduces the number of features to compare against the database. 
    	    
       	 	\citet{Lu2015} introduce a \ac{mtl} layout designed for features similarity association. Works from \citet{torralba2003context} and \citet{Ni2009} present \ac{vbl} methods that are able to localize an input query among a set of predefined places. Authors embedded the recognition process into probabilistic framework, \ac{gmm} in~\citep{torralba2003context} and epitome in~\citep{Ni2009}, trained upon images representing different areas. Such paradigms allow an easy integration of additional features (such as depth information~\citep{Ni2009}).

		\paragraph{Other Matching Methods}
	        \citet{Stumm2015a} introduce an innovative method based on graph matching. The visual vocabulary abstraction is employed and augmented with a graph of covisibility of the visual words in images. The graph is constructed as follows: nodes represent visual word detected in images and edges are created between two nodes if they are seen together in a same image. This formulation permits integrating geometric relations between the extracted features. Authors use a graph kernel for the similarity comparison among the query graph and the database~\citep{Stumm2015,Stumm2016}. Notice that graph-based approaches are often employed when scenes are described by spatially organized semantic clues such as office furnitures~\citep{Salas-Moreno2013} or street equipments~\citep{Ardeshir2014}.
        
    	    Area correlation algorithms is another approach for computing data similarity. Simple forms of correlation like Sum of Squared Difference (SSD) or Sum of Absolute Difference (SAD) have been used in \ac{vbl} to compare images~\citep{Poglitsch2015,Milford2015}. \citet{Wan2016} use PC (Phase Correlation) on images described with FT (Fourier Transform) in order to be robust to shadow artefacts. In the work of~\citep{Corke2013}, authors compare shadow invariant grey-scale images with Zero Mean Normalized Cross-Correlation (ZNCC).
        
		
	\subsection{Candidates re-ranking}
	\label{subsec:candidates_re_ranking}
		Data can be processed after the similarity research to improve the final result. Post-processing methods are widely used to re-rank the candidate list, improving relevance of retrieved data.
        		
        \paragraph{Specific VBL re-ranking}            
            Unlike conventional methods of object-retrieval, indirect \ac{vbl} can benefit from geo-localization information associated to the documents present in the database. As discussed earlier, this information can be used to construct structured graph for the similarity search process~\citep{Torii2011,Cao2013} or exploited to re-rank the candidates list~\citep{Zamir2010,Zamir2014,Sattler2016}. \citet{Zamir2010} introduce this geographic re-ranking after a classical image-retrieval algorithm to quickly remove irrelevant candidates. Authors go one step further in~\citep{Zamir2014} and embed the matching process within a Generalized Minimum Clique Graphs scheme to retrieve consistent candidates according to the GPS tag associated to the visual data. \citet{Sattler2016} generalize the problem of visual burstiness introduced by~\citep{Jegou2009} to a geographic level, introducing the concept of geometric burstiness. They improve the relevance of the ranked list of candidates using position and popularity meta-information of database images.
            
            Innovative contribution from \citet{Torii2011} refine the query location with a linear interpolation in the feature space domain of the closest database images. The database is arranged with a graph representation, where images represent the nodes and the edges encode spatial relation, i.e. images that are close to each other (according to their GPS-tag) are connected. Firstly, a set of putative candidates are retrieved with a conventional quantization method, then the discrete feature space of the candidate is extended into a continuous space by linear interpolation according to their position in the graph. The exact position of the query is then guessed according to linear combination of GPS information of the database images. Although promising, this method relies on complete panorama images, limiting its range of applications.
       
       	\paragraph{Generic re-ranking}    
			Query expansion is a post-process that re-query the database after a first retrieval step to increase the recall rate~\citep{Chum2007,Chum2011,Tolias2014}. However, increasing the recall rate is not the main concern of \ac{vbl} indirect method~\citep{Sattler2012}. Indeed, as exposed in the introduction, a perfect \ac{vbl} indirect system should retrieve at first position the closest visual document present in the database. However, more suitable top ranked candidates in the list of retrieved data could benefit to a subsequent pose estimation step~\citep{Song2016}. The \ac{vbl} system presented by \citet{Cao2013} increase the diversity of retrieved images by introducing a probabilistic re-ranking on the assumption that the first ranked candidate is not a good one and by maximizing the probability that the second one is.
			\label{par:ransac}
 			Last but not least, geometric consistency check is often used to reject wrong matching. Relative pose between the query and the database candidates is computed by considering homography or multiple-view transformation (see more details in the following \S\ref{para:pose_compute}), and candidates that produce the most consistent pose are ranked up. \citet{Philbin2007} democratize the use of spatial verification by introducing prior on the pose of the photography by assuming a top-oriented view. Authors perform spatial check hierarchically to get more flexibility between time computation and retrieval precision. The geometric transformation between the query and the candidate is usually computed with minimal algorithm embedded in random consensus, like RANSAC~\citep{Fischler1981}. There exists multiple alternatives to the classical RANSAC algorithm. PROSAC by~\citep{Chum2005}, used in~\citep{Donoser2014}, prioritize specific features during the random selection step. We can also enumerate LO-RANSAC used in~\citep{Philbin2007} and AC-RANSAC in~\citep{Qu2015,Qu2016}. Novel method F-SORT presented by \citet{Chan2016} show outstanding result both in term on matching quality and computation efficiency. Notice that these algorithms, beside improving the relevance of the retrieved candidates, can give information about the relative pose of the query. That is why numerous direct methods, presented in the next Section~\ref{sec:fine_pose_estimation}, rely also on these techniques.